{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be6a8449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0df794ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation function for audio\n",
    "def augment_audio(audio, sr, pitch_factor=2, noise_factor=0.005):\n",
    "    # Apply pitch shifting\n",
    "    audio_shifted = librosa.effects.pitch_shift(audio, sr=sr, n_steps=pitch_factor)\n",
    "    \n",
    "    # Add random noise\n",
    "    noise = np.random.randn(len(audio))\n",
    "    audio_noisy = audio_shifted + noise_factor * noise\n",
    "    \n",
    "    # Clip the audio to ensure it's within a valid range\n",
    "    audio_noisy = np.clip(audio_noisy, -1.0, 1.0)\n",
    "    \n",
    "    return audio_noisy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f3a814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the emotion from the filename\n",
    "def extract_emotion_from_filename(filename):\n",
    "    # Emotion mapping based on the filename (third component)\n",
    "    emotion_mapping = {\n",
    "        '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad', '05': 'angry', \n",
    "        '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
    "    }\n",
    "    \n",
    "    emotion_code = filename.split('-')[2]  # Extract emotion from the filename\n",
    "    return emotion_mapping.get(emotion_code, 'unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a139db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['Energy', 'MFCC_1', 'MFCC_3', 'MFCC_4', 'MFCC_6', 'MFCC_2', 'MFCC_9', \n",
    "                     'MFCC_5', 'MFCC_10', 'MFCC_13', 'ZCR', 'MFCC_7', 'MFCC_8', 'MFCC_12', \n",
    "                     'MFCC_11', 'Pitch', 'Spectral_Rolloff']\n",
    "\n",
    "\n",
    "# Function to extract the 17 selected features from audio\n",
    "def extract_selected_features(audio, sr):\n",
    "    # Feature 1: Energy\n",
    "    energy = np.sum(librosa.feature.rms(y=audio)**2, axis=1)\n",
    "    \n",
    "    # Feature 2: MFCCs (13 MFCCs)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    \n",
    "    # Feature 3: Zero Crossing Rate (ZCR)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    \n",
    "    # Feature 4: Spectral Rolloff\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "    \n",
    "    # Feature 5: Pitch (Mean pitch)\n",
    "    pitch, _ = librosa.core.piptrack(y=audio, sr=sr)\n",
    "    pitch = np.mean(pitch[pitch > 0])  # Get mean pitch\n",
    "    \n",
    "    # Extract the 17 features based on the selected ones\n",
    "    features = [\n",
    "        energy.mean(),  # Energy\n",
    "        mfccs[0, 0],  # MFCC_1\n",
    "        mfccs[2, 0],  # MFCC_3\n",
    "        mfccs[3, 0],  # MFCC_4\n",
    "        mfccs[5, 0],  # MFCC_6\n",
    "        mfccs[1, 0],  # MFCC_2\n",
    "        mfccs[8, 0],  # MFCC_9\n",
    "        mfccs[4, 0],  # MFCC_5\n",
    "        mfccs[9, 0],  # MFCC_10\n",
    "        mfccs[12, 0],  # MFCC_13\n",
    "        zcr.mean(),  # ZCR\n",
    "        mfccs[6, 0],  # MFCC_7\n",
    "        mfccs[7, 0],  # MFCC_8\n",
    "        mfccs[11, 0],  # MFCC_12\n",
    "        mfccs[10, 0],  # MFCC_11\n",
    "        pitch,  # Pitch\n",
    "        spectral_rolloff.mean()  # Spectral Rolloff\n",
    "    ]\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51b3184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to augment audio files, extract features, and save them to a CSV\n",
    "def augment_and_save_to_csv(audio_folder, output_csv='augmented_audio_features.csv', augment_count=100):\n",
    "    data = []  # List to store the data\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate through all actor folders\n",
    "    for actor_folder in os.listdir(audio_folder):\n",
    "        actor_path = os.path.join(audio_folder, actor_folder)\n",
    "        if os.path.isdir(actor_path):\n",
    "            for file in os.listdir(actor_path):\n",
    "                if file.endswith(\".wav\") and count < augment_count:\n",
    "                    file_path = os.path.join(actor_path, file)\n",
    "                    audio, sr = librosa.load(file_path, sr=None)\n",
    "                    \n",
    "                    # Apply augmentations\n",
    "                    augmented_audio = augment_audio(audio, sr)\n",
    "                    \n",
    "                    # Extract 17 features from the augmented audio\n",
    "                    features = extract_selected_features(augmented_audio, sr)\n",
    "                    \n",
    "                    # Extract emotion from filename\n",
    "                    emotion = extract_emotion_from_filename(file)\n",
    "                    \n",
    "                    # Append the features and emotion label to the data list\n",
    "                    data.append(features + [emotion])\n",
    "                    \n",
    "                    count += 1\n",
    "                    if count >= augment_count:\n",
    "                        break\n",
    "    \n",
    "    # Convert the data list to a pandas DataFrame\n",
    "    columns = selected_features + ['Emotion']  # 17 features and the emotion label\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    \n",
    "    # Save the DataFrame to CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Data saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64ec82c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to augmented_17_audio_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Specify the audio folder location\n",
    "audio_folder = 'Audio-data'  # Update with your folder path\n",
    "\n",
    "# Run the augmentation and save to CSV\n",
    "augment_and_save_to_csv(audio_folder, output_csv='augmented_17_audio_features.csv', augment_count=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce5a11",
   "metadata": {},
   "source": [
    "# Model Design 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "793ec7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc2aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the augmented data CSV\n",
    "df = pd.read_csv('feature-importance-analysis/17_selected_features.csv')\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = df.drop(columns=['Emotion'])  # Features: All columns except 'Emotion'\n",
    "y = df['Emotion']  # Labels: Emotion column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b295a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert emotion labels into numerical format using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_encoded, dtype=torch.long)\n",
    "\n",
    "# Split data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68960214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(17, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)  # Batch normalization\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)   # Batch normalization\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)   # Batch normalization\n",
    "        self.fc4 = nn.Linear(32, 8)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))  # Apply batch norm before activation\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec3fc7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = EmotionNN()\n",
    "\n",
    "# Loss function: Cross-Entropy Loss (for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Learning rate scheduler: StepLR reduces learning rate by a factor of 0.5 every 10 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  # Reduce lr by 50% every 10 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c4ae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.1089, Accuracy: 14.58%\n",
      "Epoch [2/100], Loss: 2.1101, Accuracy: 15.54%\n",
      "Epoch [3/100], Loss: 2.1131, Accuracy: 14.58%\n",
      "Epoch [4/100], Loss: 2.1093, Accuracy: 17.19%\n",
      "Epoch [5/100], Loss: 2.1221, Accuracy: 15.36%\n",
      "Epoch [6/100], Loss: 2.1268, Accuracy: 14.84%\n",
      "Epoch [7/100], Loss: 2.1320, Accuracy: 15.36%\n",
      "Epoch [8/100], Loss: 2.1003, Accuracy: 13.98%\n",
      "Epoch [9/100], Loss: 2.0962, Accuracy: 16.84%\n",
      "Epoch [10/100], Loss: 2.1254, Accuracy: 15.89%\n",
      "Epoch [11/100], Loss: 2.1211, Accuracy: 13.98%\n",
      "Epoch [12/100], Loss: 2.1350, Accuracy: 14.93%\n",
      "Epoch [13/100], Loss: 2.1219, Accuracy: 14.58%\n",
      "Epoch [14/100], Loss: 2.1342, Accuracy: 14.15%\n",
      "Epoch [15/100], Loss: 2.1032, Accuracy: 15.45%\n",
      "Epoch [16/100], Loss: 2.1230, Accuracy: 13.72%\n",
      "Epoch [17/100], Loss: 2.1379, Accuracy: 13.28%\n",
      "Epoch [18/100], Loss: 2.1162, Accuracy: 14.32%\n",
      "Epoch [19/100], Loss: 2.1119, Accuracy: 15.45%\n",
      "Epoch [20/100], Loss: 2.1078, Accuracy: 16.32%\n",
      "Epoch [21/100], Loss: 2.1186, Accuracy: 13.45%\n",
      "Epoch [22/100], Loss: 2.1167, Accuracy: 15.54%\n",
      "Epoch [23/100], Loss: 2.1099, Accuracy: 13.89%\n",
      "Epoch [24/100], Loss: 2.1240, Accuracy: 15.97%\n",
      "Epoch [25/100], Loss: 2.1195, Accuracy: 14.50%\n",
      "Epoch [26/100], Loss: 2.1176, Accuracy: 13.63%\n",
      "Epoch [27/100], Loss: 2.1166, Accuracy: 14.76%\n",
      "Epoch [28/100], Loss: 2.0943, Accuracy: 16.15%\n",
      "Epoch [29/100], Loss: 2.1247, Accuracy: 15.45%\n",
      "Epoch [30/100], Loss: 2.1169, Accuracy: 15.28%\n",
      "Epoch [31/100], Loss: 2.1164, Accuracy: 15.45%\n",
      "Epoch [32/100], Loss: 2.1219, Accuracy: 15.80%\n",
      "Epoch [33/100], Loss: 2.1183, Accuracy: 13.45%\n",
      "Epoch [34/100], Loss: 2.1476, Accuracy: 13.98%\n",
      "Epoch [35/100], Loss: 2.1202, Accuracy: 14.24%\n",
      "Epoch [36/100], Loss: 2.1151, Accuracy: 14.84%\n",
      "Epoch [37/100], Loss: 2.1168, Accuracy: 14.41%\n",
      "Epoch [38/100], Loss: 2.1171, Accuracy: 15.62%\n",
      "Epoch [39/100], Loss: 2.1111, Accuracy: 14.58%\n",
      "Epoch [40/100], Loss: 2.1039, Accuracy: 15.45%\n",
      "Epoch [41/100], Loss: 2.1164, Accuracy: 14.41%\n",
      "Epoch [42/100], Loss: 2.1133, Accuracy: 14.76%\n",
      "Epoch [43/100], Loss: 2.0978, Accuracy: 15.36%\n",
      "Epoch [44/100], Loss: 2.1172, Accuracy: 15.54%\n",
      "Epoch [45/100], Loss: 2.1201, Accuracy: 14.06%\n",
      "Epoch [46/100], Loss: 2.1284, Accuracy: 15.10%\n",
      "Epoch [47/100], Loss: 2.1089, Accuracy: 15.62%\n",
      "Epoch [48/100], Loss: 2.1162, Accuracy: 14.93%\n",
      "Epoch [49/100], Loss: 2.1220, Accuracy: 15.62%\n",
      "Epoch [50/100], Loss: 2.1242, Accuracy: 14.93%\n",
      "Epoch [51/100], Loss: 2.1250, Accuracy: 14.93%\n",
      "Epoch [52/100], Loss: 2.1130, Accuracy: 14.76%\n",
      "Epoch [53/100], Loss: 2.1204, Accuracy: 14.84%\n",
      "Epoch [54/100], Loss: 2.1143, Accuracy: 17.01%\n",
      "Epoch [55/100], Loss: 2.1168, Accuracy: 15.36%\n",
      "Epoch [56/100], Loss: 2.1166, Accuracy: 15.54%\n",
      "Epoch [57/100], Loss: 2.1104, Accuracy: 14.06%\n",
      "Epoch [58/100], Loss: 2.1159, Accuracy: 13.45%\n",
      "Epoch [59/100], Loss: 2.1235, Accuracy: 14.15%\n",
      "Epoch [60/100], Loss: 2.1104, Accuracy: 14.93%\n",
      "Epoch [61/100], Loss: 2.1068, Accuracy: 14.67%\n",
      "Epoch [62/100], Loss: 2.1448, Accuracy: 14.06%\n",
      "Epoch [63/100], Loss: 2.1138, Accuracy: 13.72%\n",
      "Epoch [64/100], Loss: 2.0995, Accuracy: 17.45%\n",
      "Epoch [65/100], Loss: 2.1353, Accuracy: 13.45%\n",
      "Epoch [66/100], Loss: 2.1448, Accuracy: 13.45%\n",
      "Epoch [67/100], Loss: 2.1293, Accuracy: 13.28%\n",
      "Epoch [68/100], Loss: 2.1084, Accuracy: 13.89%\n",
      "Epoch [69/100], Loss: 2.1222, Accuracy: 14.67%\n",
      "Epoch [70/100], Loss: 2.1122, Accuracy: 15.45%\n",
      "Epoch [71/100], Loss: 2.0969, Accuracy: 16.84%\n",
      "Epoch [72/100], Loss: 2.1185, Accuracy: 14.76%\n",
      "Epoch [73/100], Loss: 2.1184, Accuracy: 14.32%\n",
      "Epoch [74/100], Loss: 2.1113, Accuracy: 14.93%\n",
      "Epoch [75/100], Loss: 2.1164, Accuracy: 15.45%\n",
      "Epoch [76/100], Loss: 2.1260, Accuracy: 14.50%\n",
      "Epoch [77/100], Loss: 2.1117, Accuracy: 15.71%\n",
      "Epoch [78/100], Loss: 2.1239, Accuracy: 15.54%\n",
      "Epoch [79/100], Loss: 2.1320, Accuracy: 14.58%\n",
      "Epoch [80/100], Loss: 2.1248, Accuracy: 14.93%\n",
      "Epoch [81/100], Loss: 2.1289, Accuracy: 12.93%\n",
      "Epoch [82/100], Loss: 2.1179, Accuracy: 13.89%\n",
      "Epoch [83/100], Loss: 2.0910, Accuracy: 15.19%\n",
      "Epoch [84/100], Loss: 2.1269, Accuracy: 14.58%\n",
      "Epoch [85/100], Loss: 2.1233, Accuracy: 13.98%\n",
      "Epoch [86/100], Loss: 2.1443, Accuracy: 14.58%\n",
      "Epoch [87/100], Loss: 2.1039, Accuracy: 15.80%\n",
      "Epoch [88/100], Loss: 2.1126, Accuracy: 13.98%\n",
      "Epoch [89/100], Loss: 2.1037, Accuracy: 15.54%\n",
      "Epoch [90/100], Loss: 2.1179, Accuracy: 14.67%\n",
      "Epoch [91/100], Loss: 2.1243, Accuracy: 15.97%\n",
      "Epoch [92/100], Loss: 2.1021, Accuracy: 15.62%\n",
      "Epoch [93/100], Loss: 2.1191, Accuracy: 14.50%\n",
      "Epoch [94/100], Loss: 2.1372, Accuracy: 13.63%\n",
      "Epoch [95/100], Loss: 2.1067, Accuracy: 15.62%\n",
      "Epoch [96/100], Loss: 2.1038, Accuracy: 16.75%\n",
      "Epoch [97/100], Loss: 2.1184, Accuracy: 16.84%\n",
      "Epoch [98/100], Loss: 2.1174, Accuracy: 16.23%\n",
      "Epoch [99/100], Loss: 2.1103, Accuracy: 16.49%\n",
      "Epoch [100/100], Loss: 2.1064, Accuracy: 14.93%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 100  # Keep the same number of epochs\n",
    "batch_size = 32   # You can adjust this\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "    # Update learning rate using scheduler\n",
    "    scheduler.step()  # Adjust learning rate after each epoch\n",
    "\n",
    "    accuracy = 100 * correct_predictions / total_predictions\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5503185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 17.36%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    correct_predictions = (predicted == y_test).sum().item()\n",
    "    accuracy = 100 * correct_predictions / y_test.size(0)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
